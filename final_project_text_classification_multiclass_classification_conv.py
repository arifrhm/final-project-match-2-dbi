# -*- coding: utf-8 -*-
"""Final Project - Text Classification - MultiClass Classification-CONV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dXaWPEjnylcqxDQA5MQn15AmeocAV7ky

# Text Classification - CNN
"""

# load packages
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics

import keras
from keras.models import Sequential
from keras.layers import Input,Embedding, Lambda,Dropout,Conv1D,Activation, Dense, Bidirectional,GlobalMaxPooling1D, LSTM, SpatialDropout1D, TimeDistributed,Masking,Layer
from keras.models import Model
from keras.callbacks import ModelCheckpoint
import keras.backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import string
import pandas as pd
from nltk.stem.porter import PorterStemmer
import re
import spacy
from nltk.corpus import stopwords
from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS
from spacy.lang.en import English
spacy.load('en')
parser = English()

# Stop words and special characters 
STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS)) 
SYMBOLS = " ".join(string.punctuation).split(" ") + ["-", "...", "”", "”","''"]

trainDF = pd.read_csv('./bbc-text.csv')

trainDF.head(10)

trainDF.shape

# change datatype to string
trainDF = trainDF[["category","text"]].astype(str)

# check labels in category
trainDF['category'].unique()

# check counts of each category
trainDF['category'].value_counts()

sns.set(rc={'figure.figsize':(10,10)})
sns.countplot(trainDF['category'])

"""## Data preparation"""

# Data Cleaner and tokenizer
def tokenizeText(text):
    
    text = text.strip().replace("\n", " ").replace("\r", " ")
    text = text.lower()
    
    tokens = parser(text)
    
    lemmas = []
    for tok in tokens:
        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != "-PRON-" else tok.lower_)
    tokens = lemmas
    
    # reomve stop words and special charaters
    tokens = [tok for tok in tokens if tok.lower() not in STOPLIST]
    tokens = [tok for tok in tokens if tok not in SYMBOLS]
    
    tokens = [tok for tok in tokens if len(tok) >= 3]
    
    # remove remaining tokens that are not alphabetic
    tokens = [tok for tok in tokens if tok.isalpha()]
    
    tokens = [tok for tok in tokens if tok != '']
    
    # stemming of words
    #porter = PorterStemmer()
    #tokens = [porter.stem(word) for word in tokens]
    
    tokens = list(set(tokens))
    #return tokens
    return str(' '.join(tokens[:]))

# Data cleaning
trainDF['text'] = trainDF['text'].apply(lambda x:tokenizeText(x))

y = list(trainDF['category'])
x = list(trainDF['text'])

num_max = 1000

# preprocess
tok = Tokenizer(num_words=num_max)
tok.fit_on_texts(x)
vocab_size = len(tok.word_index) + 1
print(vocab_size)

# for cnn preproces
max_len = 100
cnn_texts_seq = tok.texts_to_sequences(x)
print(cnn_texts_seq[0])
print(len(cnn_texts_seq))

cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)
print(cnn_texts_mat[0])
print(cnn_texts_mat.shape)

le = preprocessing.LabelEncoder()
le.fit(y)

def encode(le, labels):
    enc = le.transform(labels)
    return keras.utils.to_categorical(enc)

def decode(le, one_hot):
    dec = np.argmax(one_hot, axis=1)
    return le.inverse_transform(dec)
y_enc = encode(le, y)

# split the dataset into training and validation datasets
x_train, x_val, y_train, y_val = model_selection.train_test_split(np.asarray(cnn_texts_mat), np.asarray(y_enc), test_size=0.2, random_state=42)

# split the dataset into training and validation datasets
x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train, y_train, test_size=0.2, random_state=42)

x_train.shape,x_val.shape, x_test.shape



"""## Model Building"""

#import tensorflow_hub as hub
import tensorflow as tf
tf.compat.v1.get_default_graph
from keras.layers.merge import add

def train_model(model,x_train,y_train,x_val,y_val,filepath):
        # checkpoint
        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
        callbacks_list = [checkpoint]
        history = model.fit(x_train, y_train,validation_data=(x_val,y_val),callbacks=callbacks_list,epochs=10, batch_size=5)
        # model.save_weights('./elmo-model.h5')

def test_model(model,x_test,file_path):
    model.load_weights(file_path)  
    predicts = model.predict(x_test, batch_size=5)
    return predicts

"""## Case 1"""

def get_cnn_model_v1():   
    model = Sequential()
    # we start off with an efficient embedding layer which maps
    # our vocab indices into embedding_dims dimensions
    model.add(Embedding(vocab_size,
                        20,
                        input_length=max_len))
    model.add(Dropout(0.2))
    model.add(Conv1D(64,
                     3,
                     padding='valid',
                     activation='relu',
                     strides=1))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(256))
    model.add(Dropout(0.2))
    model.add(Activation('relu'))
    model.add(Dense(5))
    model.add(Activation('softmax'))
    model.summary()
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model

model_v1 = get_cnn_model_v1()
    
train_model(model_v1,x_train,y_train,x_val,y_val,'./cnn_model_v1.hdf5')

predicts = test_model(model_v1,x_test,'./cnn_model_v1.hdf5')

y_test_dec = decode(le, y_test)

y_preds = decode(le, predicts)

from sklearn import metrics

print(metrics.confusion_matrix(y_test_dec, y_preds))

print(metrics.classification_report(y_test_dec, y_preds))

from sklearn.metrics import accuracy_score

print("Accuracy:",accuracy_score(y_test_dec,y_preds))

"""## Case 2"""

def get_cnn_model_v2(): # added embed   
    model = Sequential()
    # we start off with an efficient embedding layer which maps
    # our vocab indices into embedding_dims dimensions
    model.add(Embedding(vocab_size,
                        50, 
                        input_length=max_len))
    model.add(Dropout(0.2))
    model.add(Conv1D(64,
                     3,
                     padding='valid',
                     activation='relu',
                     strides=1))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(256))
    model.add(Dropout(0.2))
    model.add(Activation('relu'))
    model.add(Dense(5))
    model.add(Activation('softmax'))
    model.summary()
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model

model_v2 = get_cnn_model_v2()
train_model(model_v2,x_train,y_train,x_val,y_val,'./cnn_model_v2.hdf5')

predicts = test_model(model_v2,x_test,'./cnn_model_v2.hdf5')

y_test_dec = decode(le, y_test)

y_preds = decode(le, predicts)

from sklearn import metrics

print(metrics.confusion_matrix(y_test_dec, y_preds))

print(metrics.classification_report(y_test_dec, y_preds))

from sklearn.metrics import accuracy_score

print("Accuracy:",accuracy_score(y_test_dec,y_preds))

